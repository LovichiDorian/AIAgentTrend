"""
NÅ“uds (fonctions) du StateGraph LangGraph.

Chaque nÅ“ud prend le state en entrÃ©e et retourne les mises Ã  jour du state.
"""

import asyncio
import hashlib
import logging
from datetime import datetime
from typing import Any
from collections import defaultdict

from langchain_core.messages import HumanMessage, SystemMessage
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_mistralai import ChatMistralAI

from .state import (
    TechWatchState,
    SourceResult,
    FOCUS_SOURCE_MAPPING,
    PRIORITY_SOURCES,
    SECONDARY_SOURCES
)
from .tools import AVAILABLE_TOOLS

import os

logger = logging.getLogger(__name__)


# =============================================================================
# CONFIGURATION LLM
# =============================================================================

def get_llm(provider: str = "auto"):
    """
    Retourne le LLM configurÃ©.
    
    Args:
        provider: 'auto', 'gemini', ou 'mistral'
                  'auto' essaie Gemini d'abord, puis Mistral
    """
    gemini_key = os.getenv("GOOGLE_API_KEY")
    mistral_key = os.getenv("MISTRAL_API_KEY")
    
    if provider == "gemini" or (provider == "auto" and gemini_key):
        if gemini_key:
            return ChatGoogleGenerativeAI(
                model="gemini-2.0-flash",
                google_api_key=gemini_key,
                temperature=0.3,
                max_output_tokens=4096
            ), "gemini"
    
    if provider == "mistral" or (provider == "auto" and mistral_key):
        if mistral_key:
            return ChatMistralAI(
                model="mistral-small-latest",
                mistral_api_key=mistral_key,
                temperature=0.3,
                max_tokens=4096
            ), "mistral"
    
    return None, None


def get_available_llm_providers() -> list[str]:
    """Retourne la liste des providers LLM disponibles."""
    providers = []
    if os.getenv("GOOGLE_API_KEY"):
        providers.append("gemini")
    if os.getenv("MISTRAL_API_KEY"):
        providers.append("mistral")
    return providers


# =============================================================================
# NÅ’UD 1: PLANIFICATION
# =============================================================================

async def planning_node(state: TechWatchState) -> dict[str, Any]:
    """
    NÅ“ud de planification: dÃ©cide quels tools appeler selon la requÃªte.
    
    Analyse la requÃªte utilisateur et le focus pour dÃ©terminer:
    - Les sources prioritaires Ã  interroger
    - Les sources secondaires (si budget le permet)
    - Les paramÃ¨tres de recherche
    
    Returns:
        Mises Ã  jour du state (sources_to_query, priority_sources, etc.)
    """
    logger.info("ğŸ¯ Planification de la veille...")
    
    user_query = state.get("user_query", "").lower()
    focus = state.get("focus", "general")
    max_items = state.get("max_items_per_source", 10)
    
    # DÃ©terminer les sources selon le focus
    focus_sources = FOCUS_SOURCE_MAPPING.get(focus, FOCUS_SOURCE_MAPPING["general"])
    
    # Analyser la requÃªte pour ajuster les sources
    # Mots-clÃ©s qui influencent le choix des sources
    keywords_ai = ["ia", "ai", "llm", "gpt", "machine learning", "ml", "deep learning"]
    keywords_devops = ["devops", "kubernetes", "k8s", "docker", "cloud", "infra", "sre"]
    keywords_security = ["security", "sÃ©curitÃ©", "hack", "vulnerability", "cve"]
    keywords_web = ["web", "frontend", "backend", "react", "vue", "javascript", "typescript"]
    
    # Ajuster le focus si des mots-clÃ©s spÃ©cifiques sont dÃ©tectÃ©s
    detected_focus = focus
    if any(kw in user_query for kw in keywords_ai):
        detected_focus = "ai"
        focus_sources = FOCUS_SOURCE_MAPPING["ai"]
    elif any(kw in user_query for kw in keywords_devops):
        detected_focus = "devops"
        focus_sources = FOCUS_SOURCE_MAPPING["devops"]
    elif any(kw in user_query for kw in keywords_security):
        detected_focus = "security"
        focus_sources = FOCUS_SOURCE_MAPPING["security"]
    elif any(kw in user_query for kw in keywords_web):
        detected_focus = "web"
        focus_sources = FOCUS_SOURCE_MAPPING["web"]
    
    # SÃ©parer sources prioritaires et secondaires
    priority = [s for s in focus_sources if s in PRIORITY_SOURCES]
    secondary = [s for s in focus_sources if s in SECONDARY_SOURCES]
    
    # Ajouter des sources complÃ©mentaires si le focus est gÃ©nÃ©ral
    if detected_focus == "general" or focus == "all":
        priority = list(set(priority + ["github_trending", "hackernews"]))
        secondary = list(set(secondary + ["producthunt", "tech_news"]))
    
    logger.info(f"   Focus dÃ©tectÃ©: {detected_focus}")
    logger.info(f"   Sources prioritaires: {priority}")
    logger.info(f"   Sources secondaires: {secondary}")
    
    return {
        "sources_to_query": priority + secondary,
        "priority_sources": priority,
        "secondary_sources": secondary,
        "focus": detected_focus,
        "metadata": {
            **state.get("metadata", {}),
            "planning_completed": True,
            "detected_focus": detected_focus
        }
    }


# =============================================================================
# NÅ’UD 2: COLLECTE
# =============================================================================

async def collection_node(state: TechWatchState) -> dict[str, Any]:
    """
    NÅ“ud de collecte: interroge toutes les sources planifiÃ©es.
    
    ExÃ©cute les tools en parallÃ¨le pour optimiser le temps.
    GÃ¨re les erreurs gracieusement (une source qui Ã©choue n'arrÃªte pas le reste).
    
    Returns:
        Mises Ã  jour du state (source_results, errors)
    """
    logger.info("ğŸ“¥ Collecte des donnÃ©es...")
    
    sources_to_query = state.get("sources_to_query", [])
    priority_sources = state.get("priority_sources", [])
    
    if not sources_to_query:
        logger.warning("   Aucune source Ã  interroger!")
        return {"errors": ["Aucune source configurÃ©e pour cette requÃªte"]}
    
    source_results = {}
    errors = list(state.get("errors", []))
    api_calls = 0
    
    # D'abord les sources prioritaires
    async def fetch_source(source_name: str) -> tuple[str, SourceResult]:
        """RÃ©cupÃ¨re les donnÃ©es d'une source."""
        if source_name not in AVAILABLE_TOOLS:
            return source_name, SourceResult(
                source_name=source_name,
                items=[],
                error=f"Tool '{source_name}' non trouvÃ©"
            )
        
        try:
            tool_func = AVAILABLE_TOOLS[source_name]
            items = await tool_func()
            logger.info(f"   âœ“ {source_name}: {len(items)} items")
            return source_name, SourceResult(
                source_name=source_name,
                items=items
            )
        except Exception as e:
            error_msg = f"Erreur {source_name}: {str(e)}"
            logger.warning(f"   âœ— {error_msg}")
            return source_name, SourceResult(
                source_name=source_name,
                items=[],
                error=error_msg
            )
    
    # ExÃ©cuter les sources prioritaires d'abord
    logger.info("   Sources prioritaires...")
    priority_tasks = [fetch_source(s) for s in priority_sources if s in sources_to_query]
    priority_results = await asyncio.gather(*priority_tasks)
    
    for source_name, result in priority_results:
        source_results[source_name] = result
        api_calls += 1
        if result.error:
            errors.append(result.error)
    
    # Puis les sources secondaires
    secondary_sources = [s for s in sources_to_query if s not in priority_sources]
    if secondary_sources:
        logger.info("   Sources secondaires...")
        secondary_tasks = [fetch_source(s) for s in secondary_sources]
        secondary_results = await asyncio.gather(*secondary_tasks)
        
        for source_name, result in secondary_results:
            source_results[source_name] = result
            api_calls += 1
            if result.error:
                errors.append(result.error)
    
    total_items = sum(len(r.items) for r in source_results.values())
    logger.info(f"   Total: {total_items} items collectÃ©s depuis {len(source_results)} sources")
    
    return {
        "source_results": source_results,
        "errors": errors,
        "total_api_calls": state.get("total_api_calls", 0) + api_calls
    }


# =============================================================================
# NÅ’UD 3: FILTRAGE & DÃ‰DUPLICATION
# =============================================================================

async def filtering_node(state: TechWatchState) -> dict[str, Any]:
    """
    NÅ“ud de filtrage: dÃ©duplique et priorise les items collectÃ©s.
    
    - Supprime les doublons (mÃªme URL ou titre trÃ¨s similaire)
    - Filtre le contenu trop promotionnel
    - Trie par pertinence (score, engagement, fraÃ®cheur)
    - Limite le nombre total pour Ã©conomiser les tokens LLM
    
    Returns:
        Mises Ã  jour du state (filtered_items, deduplicated_count)
    """
    logger.info("ğŸ” Filtrage et dÃ©duplication...")
    
    source_results = state.get("source_results", {})
    max_items = state.get("max_items_per_source", 10)
    
    # AgrÃ©ger tous les items
    all_items = []
    for source_name, result in source_results.items():
        if isinstance(result, SourceResult):
            items = result.items
        elif isinstance(result, dict):
            items = result.get("items", [])
        else:
            continue
        
        for item in items:
            item["_source"] = source_name
            all_items.append(item)
    
    logger.info(f"   Items bruts: {len(all_items)}")
    
    # DÃ©duplication par URL
    seen_urls = set()
    seen_titles = set()
    unique_items = []
    
    for item in all_items:
        url = item.get("url", "")
        title = item.get("title", "").lower().strip()
        
        # Hash du titre pour comparaison approximative
        title_hash = hashlib.md5(title[:50].encode()).hexdigest()[:8]
        
        if url and url not in seen_urls and title_hash not in seen_titles:
            seen_urls.add(url)
            seen_titles.add(title_hash)
            unique_items.append(item)
    
    deduplicated_count = len(all_items) - len(unique_items)
    logger.info(f"   Doublons supprimÃ©s: {deduplicated_count}")
    
    # Filtrage du contenu promotionnel (heuristiques simples)
    promo_keywords = [
        "sponsored", "ad:", "[ad]", "promotion", "buy now",
        "discount", "coupon", "deal:", "sale:"
    ]
    
    filtered_items = []
    for item in unique_items:
        title = item.get("title", "").lower()
        if not any(kw in title for kw in promo_keywords):
            filtered_items.append(item)
    
    promo_filtered = len(unique_items) - len(filtered_items)
    if promo_filtered:
        logger.info(f"   Contenu promo filtrÃ©: {promo_filtered}")
    
    # Scoring et tri
    def compute_score(item: dict) -> float:
        """Calcule un score de pertinence pour un item."""
        score = 0.0
        
        # Engagement (upvotes, stars, comments)
        score += min(item.get("score", 0) / 100, 10)  # Cap Ã  10
        score += min(item.get("num_comments", 0) / 50, 5)  # Cap Ã  5
        
        # Bonus pour certaines sources (haute qualitÃ©)
        source = item.get("_source", "")
        if source in ["hackernews", "lobsters"]:
            score += 3
        elif source == "github_trending":
            score += 4
        elif "reddit" in source:
            score += 2
        
        # PÃ©nalitÃ© si pas de description
        if not item.get("description") and not item.get("selftext") and not item.get("summary"):
            score -= 1
        
        return score
    
    for item in filtered_items:
        item["_relevance_score"] = compute_score(item)
    
    # Trier par score dÃ©croissant
    filtered_items.sort(key=lambda x: x.get("_relevance_score", 0), reverse=True)
    
    # Limiter le nombre total (pour Ã©conomiser les tokens LLM)
    max_total_items = max_items * 5  # 5x le max par source
    if len(filtered_items) > max_total_items:
        filtered_items = filtered_items[:max_total_items]
        logger.info(f"   LimitÃ© Ã  {max_total_items} items (top pertinence)")
    
    # SÃ©parer nouveaux items et dÃ©jÃ  vus (pour Ã©viter doublons email)
    new_items = filtered_items
    seen_items = []
    
    try:
        from history_tracker import separate_new_and_seen
        new_items, seen_items = separate_new_and_seen(filtered_items)
        logger.info(f"   Nouveaux items: {len(new_items)}")
        logger.info(f"   DÃ©jÃ  vus (rappels): {len(seen_items)}")
    except ImportError:
        logger.warning("   history_tracker non disponible, tous les items traitÃ©s comme nouveaux")
    
    logger.info(f"   Items finaux: {len(filtered_items)}")
    
    return {
        "filtered_items": filtered_items,  # Tous les items pour le README
        "new_items": new_items,             # Nouveaux pour l'email
        "seen_items": seen_items,           # DÃ©jÃ  vus (rappels)
        "deduplicated_count": deduplicated_count,
        "metadata": {
            **state.get("metadata", {}),
            "filtering_completed": True,
            "items_before_filter": len(all_items),
            "items_after_filter": len(filtered_items),
            "new_items_count": len(new_items),
            "seen_items_count": len(seen_items)
        }
    }


# =============================================================================
# NÅ’UD 4: SYNTHÃˆSE LLM
# =============================================================================

SYNTHESIS_SYSTEM_PROMPT = """Tu es un assistant de veille technologique expert.
Tu dois synthÃ©tiser les informations collectÃ©es pour un dÃ©veloppeur/ingÃ©nieur full-stack, DevOps ou IA.

RÃˆGLES STRICTES:
- Utilise UNIQUEMENT les informations fournies dans les donnÃ©es
- Ne jamais inventer de liens, noms d'outils ou statistiques
- Si une information n'est pas dans les donnÃ©es, ne l'inclus pas
- Reste factuel, pas de hype exagÃ©rÃ©e
- FranÃ§ais avec termes techniques en anglais
- Phrases courtes et directes
- TOUJOURS inclure les URLs des sources entre parenthÃ¨ses ou en lien markdown
- Si des items sont marquÃ©s [RAPPEL], ils ont dÃ©jÃ  Ã©tÃ© mentionnÃ©s les jours prÃ©cÃ©dents mais restent populaires

FORMAT DE RÃ‰PONSE OBLIGATOIRE:

## ğŸ¯ Vue d'ensemble
- [3-5 points clÃ©s des grandes tendances/annonces, avec lien vers la source principale]

## ğŸ†• NouveautÃ©s de la semaine

### ğŸ› ï¸ Outils & Projets Dev
[3-10 entrÃ©es max, format:]
- **[Nom]** ([langage/stack]) - [1 phrase contexte technique] â†’ [pourquoi c'est intÃ©ressant]
  ğŸ”— [URL du repo/projet]

### ğŸ“° Articles & Discussions
[Liste des articles/posts importants avec:]
- **[Titre]** ([source]) - [1-2 phrases rÃ©sumÃ© du problÃ¨me/angle technique]
  ğŸ”— [URL de l'article]

### ğŸ¤– IA / Data / Infra
[Mises Ã  jour notables: nouveaux modÃ¨les, frameworks, annonces cloud, pricing]
  ğŸ”— [URLs des sources]

## ğŸ”„ Rappels (toujours populaires)
[Items dÃ©jÃ  mentionnÃ©s les jours prÃ©cÃ©dents mais qui restent dans le top - Ã  consulter si pas encore fait]
- **[Nom]** - [Courte description] ğŸ”— [URL]

## ğŸ“š Ã€ creuser
[3-5 recommandations concrÃ¨tes, OBLIGATOIREMENT avec les liens complets]
- [Titre ou description] â†’ [URL complÃ¨te]

---
*Sources: [liste des sources utilisÃ©es avec leurs URLs principales]*
"""


async def synthesis_node(state: TechWatchState) -> dict[str, Any]:
    """
    NÅ“ud de synthÃ¨se: gÃ©nÃ¨re la rÃ©ponse finale via LLM.
    
    StratÃ©gie pour Ã©conomiser les tokens:
    1. RÃ©sumÃ© hiÃ©rarchique (par catÃ©gorie puis global)
    2. SÃ©lection des items les plus pertinents
    3. Formatage compact des donnÃ©es avant envoi au LLM
    
    Returns:
        Mises Ã  jour du state (synthesis, section_summaries)
    """
    logger.info("âœï¸ GÃ©nÃ©ration de la synthÃ¨se...")
    
    filtered_items = state.get("filtered_items", [])
    new_items = state.get("new_items", filtered_items)  # Nouveaux items
    seen_items = state.get("seen_items", [])  # Items dÃ©jÃ  vus (rappels)
    user_query = state.get("user_query", "Quoi de neuf en tech ?")
    focus = state.get("focus", "general")
    errors = state.get("errors", [])
    
    if not filtered_items:
        return {
            "synthesis": "âŒ Aucune donnÃ©e collectÃ©e. VÃ©rifiez la configuration des sources.",
            "completed_at": datetime.now()
        }
    
    # Organiser les NOUVEAUX items par catÃ©gorie
    categories = {
        "tools": [],      # GitHub, Product Hunt, DevHunt
        "articles": [],   # HN, Reddit, Lobsters, Tech News
        "ai_data": [],    # ArXiv, Reddit ML, contenus IA
        "videos": []      # YouTube
    }
    
    # CatÃ©gorie sÃ©parÃ©e pour les rappels
    recalls = []
    
    source_category_map = {
        "github_trending": "tools",
        "producthunt": "tools",
        "devhunt": "tools",
        "hackernews": "articles",
        "lobsters": "articles",
        "tech_news": "articles",
        "reddit_programming": "articles",
        "reddit_webdev": "articles",
        "reddit_devops": "articles",
        "reddit_selfhosted": "articles",
        "reddit_netsec": "articles",
        "reddit_ml": "ai_data",
        "reddit_llm": "ai_data",
        "arxiv_ai": "ai_data",
        "youtube_tech": "videos",
        "web_search": "articles",
        "web_search_ddg": "articles"
    }
    
    # Classer les NOUVEAUX items
    for item in new_items:
        source = item.get("_source", "")
        category = source_category_map.get(source, "articles")
        categories[category].append(item)
    
    # Les items dÃ©jÃ  vus vont dans les rappels (limitÃ© Ã  10)
    recalls = seen_items[:10]
    
    # Limiter chaque catÃ©gorie pour Ã©conomiser les tokens
    max_per_category = 15
    for cat in categories:
        if len(categories[cat]) > max_per_category:
            categories[cat] = categories[cat][:max_per_category]
    
    # Formater les donnÃ©es pour le LLM (format compact)
    def format_items_for_llm(items: list[dict], category: str) -> str:
        """Formate une liste d'items de faÃ§on compacte."""
        if not items:
            return "[Aucune donnÃ©e]"
        
        lines = []
        for i, item in enumerate(items, 1):
            title = item.get("title", item.get("name", "Sans titre"))
            url = item.get("url", "")
            source = item.get("_source", "")
            
            # Infos additionnelles selon la catÃ©gorie
            extra = ""
            if category == "tools":
                lang = item.get("language", "")
                stars = item.get("total_stars", item.get("period_stars", ""))
                desc = item.get("description", item.get("tagline", ""))[:100]
                extra = f" | {lang}" if lang else ""
                extra += f" | â­{stars}" if stars else ""
                extra += f" | {desc}" if desc else ""
            elif category in ["articles", "ai_data"]:
                score = item.get("score", 0)
                comments = item.get("num_comments", 0)
                summary = item.get("summary", item.get("selftext", ""))[:150]
                extra = f" | ğŸ‘{score}" if score else ""
                extra += f" | ğŸ’¬{comments}" if comments else ""
                extra += f" | {summary}" if summary else ""
            
            lines.append(f"{i}. [{source}] {title}{extra}")
            if url:
                lines.append(f"   URL: {url}")
        
        return "\n".join(lines)
    
    # Construire le prompt
    data_section = f"""
## DonnÃ©es collectÃ©es - NOUVEAUTÃ‰S

### ğŸ› ï¸ Outils & Projets (GitHub, Product Hunt, etc.)
{format_items_for_llm(categories['tools'], 'tools')}

### ğŸ“° Articles & Discussions (HN, Reddit, Lobsters, News)
{format_items_for_llm(categories['articles'], 'articles')}

### ğŸ¤– IA / Data (ArXiv, Reddit ML)
{format_items_for_llm(categories['ai_data'], 'ai_data')}

### ğŸ¥ VidÃ©os (YouTube)
{format_items_for_llm(categories['videos'], 'videos')}

## ğŸ”„ RAPPELS (dÃ©jÃ  mentionnÃ©s les jours prÃ©cÃ©dents, toujours populaires)
{format_items_for_llm(recalls, 'articles') if recalls else "[Aucun rappel cette semaine]"}
"""
    
    # Ajouter les infos sur les erreurs si pertinent
    error_section = ""
    if errors:
        # Convertir en liste pour pouvoir slicer
        unique_errors = list(set(errors))[:5]
        error_section = f"\nâš ï¸ Sources indisponibles: {', '.join(unique_errors)}\n"
    
    user_prompt = f"""RequÃªte utilisateur: "{user_query}"
Focus: {focus}
Date: {datetime.now().strftime('%d/%m/%Y')}
{error_section}
{data_section}

GÃ©nÃ¨re une synthÃ¨se de veille tech structurÃ©e selon le format demandÃ©.
Base-toi UNIQUEMENT sur les donnÃ©es ci-dessus."""

    # Appel LLM avec fallback entre providers
    providers = get_available_llm_providers()
    
    if not providers:
        logger.warning("   âš ï¸ Aucun LLM configurÃ©, mode dÃ©gradÃ©")
        fallback_synthesis = generate_fallback_synthesis(categories, errors)
        return {
            "synthesis": fallback_synthesis,
            "errors": state.get("errors", []) + ["Aucun LLM configurÃ©"],
            "section_summaries": {
                "tools_count": len(categories["tools"]),
                "articles_count": len(categories["articles"]),
                "ai_data_count": len(categories["ai_data"]),
                "videos_count": len(categories["videos"])
            },
            "completed_at": datetime.now()
        }
    
    messages = [
        SystemMessage(content=SYNTHESIS_SYSTEM_PROMPT),
        HumanMessage(content=user_prompt)
    ]
    
    last_error = None
    for provider in providers:
        try:
            llm, provider_name = get_llm(provider)
            if not llm:
                continue
                
            logger.info(f"   Appel LLM ({provider_name}) en cours...")
            response = await llm.ainvoke(messages)
            synthesis = response.content
            
            logger.info(f"   âœ“ SynthÃ¨se gÃ©nÃ©rÃ©e via {provider_name}")
            
            return {
                "synthesis": synthesis,
                "section_summaries": {
                    "tools_count": len(categories["tools"]),
                    "articles_count": len(categories["articles"]),
                    "ai_data_count": len(categories["ai_data"]),
                    "videos_count": len(categories["videos"])
                },
                "total_api_calls": state.get("total_api_calls", 0) + 1,
                "completed_at": datetime.now(),
                "metadata": {
                    **state.get("metadata", {}),
                    "llm_provider": provider_name
                }
            }
            
        except Exception as e:
            last_error = str(e)
            logger.warning(f"   âš ï¸ {provider} Ã©chouÃ©: {last_error[:100]}...")
            # Essayer le provider suivant
            continue
    
    # Tous les providers ont Ã©chouÃ©
    error_msg = f"Tous les LLMs ont Ã©chouÃ©. DerniÃ¨re erreur: {last_error}"
    logger.error(f"   âœ— {error_msg}")
    
    # Fallback: gÃ©nÃ©rer un rÃ©sumÃ© basique sans LLM
    fallback_synthesis = generate_fallback_synthesis(categories, errors)
    
    return {
        "synthesis": fallback_synthesis,
        "errors": state.get("errors", []) + [error_msg],
        "section_summaries": {
            "tools_count": len(categories["tools"]),
            "articles_count": len(categories["articles"]),
            "ai_data_count": len(categories["ai_data"]),
            "videos_count": len(categories["videos"])
        },
        "completed_at": datetime.now()
    }


def generate_fallback_synthesis(categories: dict, errors: list) -> str:
    """GÃ©nÃ¨re une synthÃ¨se basique sans LLM (fallback)."""
    
    lines = ["# ğŸ“Š Veille Tech (Mode dÃ©gradÃ© - sans LLM)\n"]
    
    if errors:
        unique_errors = list(set(str(e)[:50] for e in errors))[:3]
        lines.append(f"âš ï¸ Erreurs: {', '.join(unique_errors)}\n")
    
    # Section Outils/Projets
    tools = categories.get("tools", [])
    if tools:
        lines.append("## ğŸ› ï¸ Top Outils/Projets\n")
        for item in tools[:8]:
            name = item.get("name", item.get("title", "?"))
            url = item.get("url", "")
            lang = item.get("language", "")
            desc = item.get("description", item.get("tagline", ""))[:80]
            stars = item.get("total_stars", item.get("period_stars", ""))
            
            line = f"- **{name}**"
            if lang:
                line += f" ({lang})"
            if stars:
                line += f" â­{stars}"
            if desc:
                line += f" - {desc}"
            if url:
                line += f"\n  â†’ {url}"
            lines.append(line)
    
    # Section Articles/Discussions
    articles = categories.get("articles", [])
    if articles:
        lines.append("\n## ğŸ“° Top Articles/Discussions\n")
        for item in articles[:10]:
            title = item.get("title", "?")[:80]
            url = item.get("url", "")
            score = item.get("score", 0)
            source = item.get("_source", item.get("source_name", ""))
            comments = item.get("num_comments", 0)
            
            line = f"- **{title}**"
            if source:
                line += f" [{source}]"
            if score:
                line += f" ğŸ‘{score}"
            if comments:
                line += f" ğŸ’¬{comments}"
            if url:
                line += f"\n  â†’ {url}"
            lines.append(line)
    
    # Section IA/Data
    ai_data = categories.get("ai_data", [])
    if ai_data:
        lines.append("\n## ğŸ¤– IA/Data\n")
        for item in ai_data[:5]:
            title = item.get("title", "?")[:80]
            url = item.get("url", "")
            source = item.get("_source", "")
            
            line = f"- {title}"
            if source:
                line += f" [{source}]"
            if url:
                line += f"\n  â†’ {url}"
            lines.append(line)
    
    # Section VidÃ©os
    videos = categories.get("videos", [])
    if videos:
        lines.append("\n## ğŸ¥ VidÃ©os\n")
        for item in videos[:3]:
            title = item.get("title", "?")[:60]
            channel = item.get("channel", "")
            url = item.get("url", "")
            
            line = f"- {title}"
            if channel:
                line += f" ({channel})"
            if url:
                line += f"\n  â†’ {url}"
            lines.append(line)
    
    lines.append("\n---\n*SynthÃ¨se gÃ©nÃ©rÃ©e en mode dÃ©gradÃ© (LLM indisponible ou quota dÃ©passÃ©)*")
    lines.append("*ğŸ’¡ Astuce: Configurez MISTRAL_API_KEY comme backup dans .env*")
    
    return "\n".join(lines)


# =============================================================================
# NÅ’UD FINAL: FORMATAGE
# =============================================================================

async def output_node(state: TechWatchState) -> dict[str, Any]:
    """
    NÅ“ud final: prÃ©pare la sortie et les mÃ©tadonnÃ©es finales.
    """
    logger.info("ğŸ“¤ Finalisation...")
    
    synthesis = state.get("synthesis", "")
    new_items = state.get("new_items", [])
    seen_items = state.get("seen_items", [])
    
    # Marquer les nouveaux items comme envoyÃ©s dans l'historique
    try:
        from history_tracker import mark_items_as_sent
        mark_items_as_sent(new_items)
        logger.info(f"   {len(new_items)} items marquÃ©s comme envoyÃ©s")
    except ImportError:
        pass
    
    # Ajouter un footer avec les stats
    stats = state.get("section_summaries", {})
    api_calls = state.get("total_api_calls", 0)
    errors = state.get("errors", [])
    
    footer = f"""

---
ğŸ“Š **Stats**: {stats.get('tools_count', 0)} outils | {stats.get('articles_count', 0)} articles | {stats.get('ai_data_count', 0)} IA/data | {stats.get('videos_count', 0)} vidÃ©os
ğŸ†• **NouveautÃ©s**: {len(new_items)} | ğŸ”„ **Rappels**: {len(seen_items)}
ğŸ”„ **Appels API**: {api_calls}
"""
    
    if errors:
        footer += f"âš ï¸ **Erreurs**: {len(errors)} source(s) indisponible(s)\n"
    
    return {
        "synthesis": synthesis + footer,
        "completed_at": datetime.now(),
        "metadata": {
            **state.get("metadata", {}),
            "completed": True,
            "total_items_processed": len(state.get("filtered_items", [])),
            "new_items_count": len(new_items),
            "seen_items_count": len(seen_items),
            "total_api_calls": api_calls,
            "error_count": len(errors)
        }
    }
